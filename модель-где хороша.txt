=== Картинки (CV) ===
1. CNN (Convolutional Neural Network)
    Смотрит на картинку кусками (фильтрами), выделяет края, формы, текстуры.
    Классификация изображений, детекция объектов, сегментация.

2. 1D-CNN
   Та же идея, но свёртка по линии (последовательности).
   Временные ряды, ДНК, сигналы.

3. ResNet / EfficientNet
   Улучшенные CNN: ResNet "перепрыгивает" слои (skip connections), EfficientNet оптимизирует размер сети.
   Сложные датасеты, глубокие модели, работа с большими изображениями.

4. Vision Transformer (ViT)
   Делит картинку на патчи и обрабатывает их как "слова" в тексте через attention.
   Большие датасеты, глобальное понимание изображения, мультимодальные задачи.

5. MLP (многослойный перцептрон)
   Полносвязные слои: каждый нейрон соединён со всеми входами.
   Простые изображения, маленькие датасеты.

---

=== Табличные данные ===
1. Logistic Regression
   Линейная граница: строит прямую/гиперплоскость, чтобы разделить классы.
   Бинарная классификация, быстрые базовые модели.

2. Decision Tree
   Строит дерево условий (if-else) для разделения объектов.
   Интерпретируемые модели, простые таблицы.

3. Random Forest
   Делает много деревьев и берёт "голосование".
   Снижение переобучения, задачи среднего размера.

4. Gradient Boosting (XGBoost, CatBoost, LightGBM)
   Строит деревья одно за другим, каждое исправляет ошибки предыдущих.
   Kaggle, финансы, медицина, большие таблицы.

5. MLP
   Нейросеть с полносвязными слоями.
   Когда признаков много и есть нелинейные зависимости.

---

=== Текст (NLP) ===
1. Bag of Words / TF-IDF + Logistic Regression
   Преобразует текст в "мешок слов" или частоты слов, потом линейная модель.
   Классификация текста, спам-фильтры.

2. Word Embeddings (Word2Vec, GloVe, FastText)
   Каждое слово кодируется как вектор, близкие слова → близкие вектора.
   Семантический поиск, подготовка данных для нейросетей.

3. RNN / LSTM / GRU
   Читают текст по порядку, сохраняют "память" о предыдущих словах.
   Генерация текста, анализ тональности, последовательный ввод.

4. Transformers (BERT, GPT)
   Attention: каждая часть текста "видит" остальные, понимает связи.
   Поиск, перевод, чат-боты, суммаризация.

---

=== Последовательности, био, временные ряды ===
1. RNN / LSTM / GRU
   Обрабатывают данные шаг за шагом, помнят контекст.
   Временные ряды, ДНК, речь.

2. 1D-CNN
   Находит локальные паттерны в последовательностях.
   Биоинформатика, сигналы, аудио.

3. Transformers для time series
   Attention на длинных рядах, ловит глобальные закономерности.
   Длинные временные ряды, сложные био-данные.

4. MLP
   Полносвязные слои, работает с заранее извлечёнными признаками.
   Простые прогнозы, табличные временные данные.
